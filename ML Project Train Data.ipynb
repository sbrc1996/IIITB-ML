{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING ESSENTIAL LIBARARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing Essential Libraries...')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing classes from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the sklean classes\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_Regression  = LogisticRegression(max_iter=10000)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the data.....')\n",
    "df = pd.read_csv('./Data/train_data.csv')\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first 5 rows are: ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The data has the following information: ')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the dataframe is:- Rows: ',df.shape[0],' Columns: ',df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization and Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the given data how many loans replayed and defaulted loans.\n",
    "# fig, ax = plt.subplots(figsize=(7,5))\n",
    "# sns.countplot(x = 'TARGET',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who is the highest borrower? Male or Female?\n",
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# sns.countplot(x='CODE_GENDER',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #How is the distribution of target labels? - Did most people return on time ?\n",
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# sns.countplot(x ='TARGET',data=df, hue='TARGET',palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Whether is it Female who has more difficulties or is it Male in repaying the loan?\n",
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# sns.countplot(x='TARGET',hue='CODE_GENDER',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Who owns most number of the cars? M or F?\n",
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# sns.countplot(x='CODE_GENDER', hue='FLAG_OWN_CAR', data=df,palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].nunique() < 100:\n",
    "        l1.append(i)\n",
    "    else:\n",
    "        l2.append(i)\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Drawing Histogram!!!')\n",
    "# for h in l2:\n",
    "#     fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "#     sns.histplot(df[h], palette='Blues_r')\n",
    "#     fig.text(0.1, 0.95, f'{h}', fontsize=16, fontweight='bold', fontfamily='serif')\n",
    "#     plt.xlabel('value ', fontsize=10)\n",
    "#     plt.ylabel('count',fontsize=10)\n",
    "#     plt.yticks(fontsize=13)\n",
    "#     plt.box(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING THE ENTIRE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING THE COLUMNS THAT HAVE NULL VALUES OF MORE THAN 50%\n",
    "### DROP THEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[] #this is a list that stores the names of cols having more than 50% nulls\n",
    "for features in df.columns:\n",
    "    percentage = (df[features].isna().sum()/df.shape[0]) *100\n",
    "    if df[features].isna().sum() > 0 and percentage > 50.0:\n",
    "        to_drop.append(features)\n",
    "        print(features,'    ' ,df[features].isna().sum(), percentage)\n",
    "        df.drop(features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the dataframe after deleting the columns are:- Rows: ',df.shape[0],' Columns: ',df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the SK_ID_CURR column as it a primary key and has no actual use here.\n",
    "df.drop('SK_ID_CURR',axis= 1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate data rows\n",
    "countDuplicateRows = df[df.duplicated(subset = None, keep= False)].shape[0]\n",
    "print('The number of Duplicate Rows present here are: ',countDuplicateRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAKING CARE OF NULL VALUES IN THE REMAINING COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have deleted the columns with >50% Null values, but there are some columns with null values. \n",
    "#Handling them here.\n",
    "#Checking the remaining columns with null values and their corresponding median and mean values and store them in a list.\n",
    "\n",
    "count = 0\n",
    "featureList = []\n",
    "for features in df.columns:\n",
    "    if df[features].isna().sum() > 0 and df[features].dtype != object:\n",
    "        featureList.append(features)\n",
    "        print(features, '   ', df[features].isna().sum(),'  ',df[features].median(), '  ',df[features].mean() )\n",
    "        count += 1\n",
    "print('\\n\\nTotal such columns are',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the List that we got here.(i.e names of features having some null values)\n",
    "print(featureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the list obtained in the previous cell, we have to pick out those who have integer numbers and then fill the rest i.e float values. \n",
    "#ones with median values of the entire column.\n",
    "\n",
    "\n",
    "modifiedListFeaturesWithWholeNumbers = ['CNT_FAM_MEMBERS','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "\n",
    "s = set()\n",
    "for i in modifiedListFeaturesWithWholeNumbers:\n",
    "    s.add(i)\n",
    "\n",
    "#Contains only the list with floating point values.\n",
    "newList = []\n",
    "for i in featureList:\n",
    "    if i not in s:\n",
    "       newList.append(i) \n",
    "print(newList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the null values in columns with floating point values and fill it out with median of the entire column.\n",
    "for i in newList:\n",
    "    df[i] = df[i].fillna(df[i].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The left out columns of the categorical type and integer numbers.\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].isna().sum() > 0:\n",
    "        print(i,df[i].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the elements in modifiedListFeaturesWithWholeNumbers with integers.\n",
    "for i in modifiedListFeaturesWithWholeNumbers:\n",
    "    if df[i].dtype != object:\n",
    "        df[i] = df[i].fillna(int(df[i].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The columns left out with still null values. These are mainly of type string/object.\n",
    "categoricalData = []\n",
    "for i in df.columns:\n",
    "    if df[i].isna().sum() > 0:\n",
    "        print(i,df[i].dtype,df[i].isna().sum())\n",
    "        categoricalData.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the Columns that have values that don't make any sense.\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        print(i,'   ',df[i].unique(),'  ',df[i].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns have values that don't make any sense.\n",
    "\n",
    "print(df['CODE_GENDER'].unique())               # XNA\n",
    "print(df['NAME_TYPE_SUITE'].unique())           # Other_A, Other_B\n",
    "print(df['NAME_FAMILY_STATUS'].unique())        # Unknown\n",
    "print(df['ORGANIZATION_TYPE'].unique())         # XNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing these Absurd values with NaN values.\n",
    "\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace('XNA',np.nan)\n",
    "df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].replace('Other_A',np.nan)\n",
    "df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].replace('Other_B',np.nan)\n",
    "df['NAME_FAMILY_STATUS'] = df['NAME_FAMILY_STATUS'].replace('Unknown',np.nan)\n",
    "df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].replace('XNA',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling the nulls in categorical data.\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        df[i] = df[i].fillna(df[i].mode()[0])\n",
    "\n",
    "#NULLS HAVE BEEN REMOVED.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAYS_LAST_PHONE_CHANGE      #contians negative date values\n",
    "\n",
    "#Need to handle this also. Can't understand what this -ve value means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABEL ENCODING FOR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding all the categorical object using Label Encoder.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print('Applying Label Encoding....')\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        df[i] = label_encoder.fit_transform(df[i])\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('TARGET',axis = 1)\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDLING THE IMBALANCED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import KMeansSMOTE,SMOTE,ADASYN,SVMSMOTE\n",
    "ksmote = ADASYN(0.75,random_state=42)\n",
    "X_res,Y_res = ksmote.fit_resample(x,y)\n",
    "\n",
    "\n",
    "print(\"The number of classes before fit {}\",format(Counter(y)))\n",
    "print(\"The number of classes after fit {}\",format(Counter(Y_res)))\n",
    "\n",
    "print('Shape before sampling',x.shape,y.shape)\n",
    "print('Shape after sampling',X_res.shape,Y_res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN TEST SPLIT THE ENTIRE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_Train,X_Test,Y_Train,Y_Test = train_test_split(X_res,Y_res,train_size= 0.7,shuffle= True,random_state=42)\n",
    "print(\"Training Data shape:  \",X_Train.shape,Y_Train.shape)\n",
    "print(\"Testing  Data shape:  \",X_Test.shape,Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the correlation of the inpendent features with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "abs_corr_matrix = X_Train.corr().abs()                               #absoulte value correlation matrix\n",
    "corr_target_checker = abs_corr_matrix[len(abs_corr_matrix)-1:]  #corr of target\n",
    "corr_target_checker\n",
    "to_drop=[]                                                      # A list that stores col names of low corr\n",
    "count = 0\n",
    "for i in corr_target_checker.columns:\n",
    "    if corr_target_checker[i].sum() < 0.0005:\n",
    "        print('Value: ',corr_target_checker[i].sum(),'Feature Name: ',i)\n",
    "        to_drop.append(i)\n",
    "        count += 1\n",
    "print(count)\n",
    "print(to_drop)\n",
    "\n",
    "print('Dropping Columns....')\n",
    "print(X_Train.shape)\n",
    "X_Train.drop(to_drop,axis= 1,inplace= True)\n",
    "print(X_Train.shape)\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION USING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# For Random Forest Classifier\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators= 100,criterion= 'entropy'))\n",
    "sel.fit(X_Train,Y_Train)\n",
    "\n",
    "\n",
    "# # For Decision Tree Classifier\n",
    "# sel = SelectFromModel(ExtraTreesClassifier(n_estimators= 100,criterion= 'gini'))\n",
    "# sel.fit(X_Train,Y_Train)\n",
    "\n",
    "\n",
    "selected_features = X_Train.columns[(sel.get_support())]\n",
    "print(\"The Number of features selected are: \",len(selected_features))\n",
    "print(\"The features selected are: \",selected_features)\n",
    "\n",
    "# pd.series(sel.estimator_,feature_importa).hist()\n",
    "\n",
    "#Dropping the columns that are not present in the selected_features list\n",
    "\n",
    "for i in X_Train.columns:\n",
    "    if i not in selected_features:\n",
    "        X_Train.drop(i,axis= 1,inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the selected_features for doing EDA on the Test Data.\n",
    "%store selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers Using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting BoxPlot of the left out columns to check if they have any outliers.\n",
    "# print('Before Removing Outliers')\n",
    "# count = 0\n",
    "# for i in X_Train.columns:\n",
    "#     fig = plt.figure(figsize=(7,4))\n",
    "#     plt.boxplot(X_Train[i])\n",
    "#     plt.suptitle(i)\n",
    "#     plt.show()\n",
    "#     count += 1\n",
    "\n",
    "# print('Total Boxplots printed are: ',count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now find the outliers using IQR Method and remove them from the entire data X_Train\n",
    "def drop_outliers_IQR(df,feature):\n",
    "    iqr = 1.5*( np.percentile(df[feature],75) - np.percentile(df[feature],25)   )\n",
    "    # df.drop( df[df[feature] > (iqr + np.percentile(df[feature],75))].index,inplace= True )\n",
    "    # df.drop( df[df[feature] < (np.percentile(df[feature],25) - iqr)].index,inplace= True )\n",
    "\n",
    "    df.loc[df[feature] > (iqr + np.percentile(df[feature],75))] = df[feature].mean()\n",
    "    df.loc[df[feature] < (np.percentile(df[feature],25) - iqr)] = df[feature].mean()\n",
    "    #df[feature] = df[feature].fillna(df[feature].mean())\n",
    "   \n",
    "\n",
    "for i in X_Train.columns:\n",
    "    drop_outliers_IQR(X_Train,i)\n",
    "\n",
    "\n",
    "# drop_outliers_IQR(X_Train,'CNT_CHILDREN')\n",
    "# drop_outliers_IQR(X_Train,'NAME_FAMILY_STATUS')\n",
    "# drop_outliers_IQR(X_Train,'NAME_HOUSING_TYPE')\n",
    "# drop_outliers_IQR(X_Train,'OCCUPATION_TYPE')\n",
    "# drop_outliers_IQR(X_Train,'CNT_FAM_MEMBERS')\n",
    "# drop_outliers_IQR(X_Train,'HOUR_APPR_PROCESS_START')\n",
    "# drop_outliers_IQR(X_Train,'EXT_SOURCE_1')\n",
    "# drop_outliers_IQR(X_Train,'EXT_SOURCE_3')\n",
    "# drop_outliers_IQR(X_Train, 'ENTRANCES_MODE')\n",
    "# drop_outliers_IQR(X_Train,'FLOORSMAX_MODE')\n",
    "# drop_outliers_IQR(X_Train,'FLOORSMAX_MEDI')\n",
    "# drop_outliers_IQR(X_Train,'WALLSMATERIAL_MODE')\n",
    "# drop_outliers_IQR(X_Train,'DEF_30_CNT_SOCIAL_CIRCLE')\n",
    "# drop_outliers_IQR(X_Train,'OBS_60_CNT_SOCIAL_CIRCLE')\n",
    "# drop_outliers_IQR(X_Train,'DEF_60_CNT_SOCIAL_CIRCLE')\n",
    "# drop_outliers_IQR(X_Train,'AMT_REQ_CREDIT_BUREAU_DAY')\n",
    "# drop_outliers_IQR(X_Train,'AMT_REQ_CREDIT_BUREAU_WEEK')\n",
    "# drop_outliers_IQR(X_Train,'AMT_REQ_CREDIT_BUREAU_QRT')\n",
    "# drop_outliers_IQR(X_Train,'AMT_REQ_CREDIT_BUREAU_YEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting BoxPlot of the left out columns after removal of outliers.\n",
    "# print('After Removing Outliers')\n",
    "# count = 0\n",
    "# for i in X_Train.columns:\n",
    "#     fig = plt.figure(figsize=(7,4))\n",
    "#     plt.boxplot(X_Train[i])\n",
    "#     plt.suptitle(i)\n",
    "#     plt.show()\n",
    "#     count += 1\n",
    "\n",
    "# print('Total Boxplots printed are: ',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Train.shape,Y_Train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING THE COLUMNS FROM THE TEST DATA PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_Test.columns:\n",
    "    if i not in selected_features:\n",
    "        X_Test.drop(i,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALING THE ENTIRE DATA USING STANDARD SCALAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying Scaling on the training data only for the features...')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_Train)\n",
    "X_Train_scaled = scaler.transform(X_Train)\n",
    "print('Done!!')\n",
    "#Pass this scaled data as input to the Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_Regression.fit(X_Train,Y_Train)\n",
    "Y_Pred = logistic_Regression.predict(X_Test)\n",
    "\n",
    "lacc = accuracy_score(Y_Pred,Y_Test)\n",
    "lf1 = f1_score(Y_Pred,Y_Test)\n",
    "lauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "print('The accuracy of the model on training data is: ')\n",
    "\n",
    "print('The accuracy  is: ',lacc*100,'%')\n",
    "print('The value of f1_score is: ',lf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',lauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING K NEAREST NEIGHBOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# for k in range(1, 100, 5):\n",
    "#     k = k + 1\n",
    "#     knn = KNeighborsClassifier(n_neighbors = k).fit(X_Train,Y_Train)\n",
    "#     acc = knn.score(X_Test,Y_Test)\n",
    "#     print(\"Accuracy for k = \",k,\" is: \",acc)\n",
    "\n",
    "# Here we are selecting which is the best n value for the KNN algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors= 2).fit(X_Train,Y_Train)\n",
    "Y_Pred = knn.predict(X_Test)\n",
    "kacc = accuracy_score(Y_Pred,Y_Test)\n",
    "kf1 = f1_score(Y_Pred,Y_Test)\n",
    "kauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',kacc*100,'%')\n",
    "print('The value of f1_score is: ',kf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',kauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING SUPPORT VECTOR MACHINES(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# svc = SVC()\n",
    "# svc.fit(X_Train,Y_Train)\n",
    "\n",
    "# Y_Pred = svc.predict(X_Test)\n",
    "# acc = accuracy_score(Y_Pred,Y_Test)\n",
    "# f1 = f1_score(Y_Pred,Y_Test)\n",
    "# auc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "# print('The accuracy of the model on Data is: ')\n",
    "\n",
    "# print('The accuracy  is: ',acc*100,'%')\n",
    "# print('The value of f1_score is: ',f1*100,'%')\n",
    "# print('The value of Roc AUC Score is: ',auc_score*100,'%')\n",
    "\n",
    "# print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "# print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING XGBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applying Grid Search CV to get the best hyperparameters for the XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = XGBClassifier(random_state = 30)\n",
    "# search_space = {\n",
    "#     \"n_estimators\" :   [100,200],\n",
    "#     \"max_depth\" :      [3,6,7],\n",
    "#     \"gamma\" :          [0.01,0.1],\n",
    "#     \"learning_rate\" :  [0.001,0.01,0.1,1]\n",
    "# }\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# GS = GridSearchCV(\n",
    "#     estimator= xgb_model,\n",
    "#     param_grid= search_space,\n",
    "#     scoring= [\"roc_auc\",\"roc_auc_ovr\",\"roc_auc_ovo\",\"f1\",\"f1_micro\",\"f1_macro\",\"accuracy\"],\n",
    "#     refit= \"roc_auc\",\n",
    "#     cv= 5,\n",
    "#     verbose= 4\n",
    "# )\n",
    "\n",
    "# GS.fit(X_Train,Y_Train)\n",
    "\n",
    "# print(\"The best estimator is: \",GS.best_estimator_)\n",
    "# print(\"The best parameter is: \",GS.best_params_)\n",
    "# print(\"The best AUC_ROC score is: \",GS.best_score_)\n",
    "# df_XGBoost = pd.DataFrame(GS.cv_results_)\n",
    "# df_XGBoost = df_XGBoost.sort_values(\"rank_test_roc_auc\")\n",
    "# df_XGBoost.to_csv('./Test_Output/XGBoost_GridSearchCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              early_stopping_rounds=None, enable_categorical=False,\n",
    "              eval_metric=None, feature_types=None, gamma=0.1, gpu_id=-1,\n",
    "              grow_policy='depthwise', importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
    "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "              max_depth=7, max_leaves=0, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=30)\n",
    "model_XGB.fit(X_Train,Y_Train)\n",
    "Y_Pred = model_XGB.predict(X_Test)\n",
    "xacc = accuracy_score(Y_Pred,Y_Test)\n",
    "xf1 = f1_score(Y_Pred,Y_Test)\n",
    "xauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',xacc*100,'%')\n",
    "print('The value of f1_score is: ',xf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',xauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_Train,Y_Train)\n",
    "Y_Pred = dt.predict(X_Test)\n",
    "dacc = accuracy_score(Y_Pred,Y_Test)\n",
    "df1 = f1_score(Y_Pred,Y_Test)\n",
    "dauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',dacc*100,'%')\n",
    "print('The value of f1_score is: ',df1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',dauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(X_Train,Y_Train)\n",
    "Y_Pred = rf.predict(X_Test)\n",
    "racc = accuracy_score(Y_Pred,Y_Test)\n",
    "rf1 = f1_score(Y_Pred,Y_Test)\n",
    "rauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',racc*100,'%')\n",
    "print('The value of f1_score is: ',rf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',rauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING VARIOUS NAIVE BAYES ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GAUSSIAN NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_Train,Y_Train)\n",
    "Y_Pred = model.predict(X_Test)\n",
    "gnbacc = accuracy_score(Y_Pred,Y_Test)\n",
    "gnbf1 = f1_score(Y_Pred,Y_Test)\n",
    "gnbauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',gnbacc*100,'%')\n",
    "print('The value of f1_score is: ',gnbf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',gnbauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERNOULLI NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_Train,Y_Train)\n",
    "Y_Pred = model.predict(X_Test)\n",
    "bnbacc = accuracy_score(Y_Pred,Y_Test)\n",
    "bnbf1 = f1_score(Y_Pred,Y_Test)\n",
    "bnbauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',bnbacc*100,'%')\n",
    "print('The value of f1_score is: ',bnbf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',bnbauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARING ALL THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "t = PrettyTable(['ALGO','ACCURACY SCORE','F1 SCORE','ROC-AUC SCORE'])\n",
    "t.add_row(['Logistic Regression',round(lacc,2),round(lf1,2),round(lauc_score,2)])\n",
    "t.add_row(['KNN',round(kacc,2),round(kf1,2),round(kauc_score,2)])\n",
    "t.add_row(['XG Boost',round(xacc,2),round(xf1,2),round(xauc_score,2)])\n",
    "t.add_row(['Decision Trees',round(dacc,2),round(df1,2),round(dauc_score,2)])\n",
    "t.add_row(['Random Forest',round(racc,2),round(rf1,2),round(rauc_score,2)])\n",
    "t.add_row(['Gaussian NB', round(gnbacc,2),round(gnbf1,2),round(gnbauc_score,2)])\n",
    "t.add_row(['Bernoulli NB',round(bnbacc,2),round(bnbf1,2),round(bnbauc_score,2)])\n",
    "\n",
    "\n",
    "print(t)\n",
    "\n",
    "#Looking at the values it can be concluded that the best algo is XGBOOST!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the test data.....')\n",
    "df_test = pd.read_csv('./Data/test_data.csv')\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = df_test['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the unnecessary columns from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns that are not in X_Train\n",
    "for i in df_test.columns:\n",
    "    if i not in selected_features:\n",
    "        df_test.drop(i,axis=1,inplace= True)\n",
    "\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Unnecessary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype == 'object':\n",
    "        print(\"Column: \",i,\" Values: \",df_test[i].unique())\n",
    "\n",
    "df_test['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].replace('XNA',np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype == 'object':\n",
    "        df_test[i] = df_test[i].fillna(df_test[i].mode()[0])\n",
    "    if df_test[i].dtype == 'int64':\n",
    "        df_test[i] = df_test[i].fillna(int(df_test[i].mean()))\n",
    "    if df_test[i].dtype == 'float64':\n",
    "        df_test[i] = df_test[i].fillna(df_test[i].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the categorical and non-categorical data\n",
    "cat = []\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype == 'object':\n",
    "        cat.append(i)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "label_encoder_test = preprocessing.LabelEncoder()\n",
    "\n",
    "print('Applying Label Encoding to only Categorical Data....')\n",
    "for i in cat:\n",
    "    df_test[i] = label_encoder_test.fit_transform(df_test[i])\n",
    "\n",
    "print('Done!!!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply OutLier Removing from non categorical columns in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers_IQR(df,feature):\n",
    "    iqr = 1.5*( np.percentile(df[feature],75) - np.percentile(df[feature],25)   )\n",
    "    # df.drop( df[df[feature] > (iqr + np.percentile(df[feature],75))].index,inplace= True )\n",
    "    # df.drop( df[df[feature] < (np.percentile(df[feature],25) - iqr)].index,inplace= True )\n",
    "\n",
    "    df.loc[df[feature] > (iqr + np.percentile(df[feature],75))] = df[feature].mean()\n",
    "    df.loc[df[feature] < (np.percentile(df[feature],25) - iqr)] = df[feature].mean()\n",
    "    #df[feature] = df[feature].fillna(df[feature].mean())\n",
    "   \n",
    "\n",
    "for i in df_test.columns:\n",
    "    if i not in cat:\n",
    "        drop_outliers_IQR(df_test,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The test data is ready and could be now applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets apply the best model for which we got the highest AUC_ROC Score i.e XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Pred_final = model_XGB.predict(df_test)\n",
    "\n",
    "# Here Y_Pred_final is the final data prepared by us now we need to upload it to  kaggle.\n",
    "\n",
    "\n",
    "print(type(Y_Pred_final))\n",
    "print(type(id_column))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Data for Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle = id_column.to_frame()\n",
    "\n",
    "df_kaggle['TARGET'] = Y_Pred_final.tolist()\n",
    "\n",
    "df_kaggle.to_csv('./Test_Output/Submission_XGB.csv',index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
